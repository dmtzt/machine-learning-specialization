{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbjXnTwzEO42X9egBOJTFz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmtzt/machine-learning-specialization/blob/main/2_multiclass_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiclass Classification\n",
        "The target $y$ can take on more than two possible values."
      ],
      "metadata": {
        "id": "md_7WMFW6Klg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation functions\n",
        "- Linear activation function\n",
        "- Sigmoid\n",
        "- ReLU: **Re**ctified **L**inear **U**nit\n",
        "\n",
        "### Choosing an activation function\n",
        "#### Output layer\n",
        "| Problem                 | Function |\n",
        "|-------------------------|----------|\n",
        "| Binary classification   | Sigmoid  |\n",
        "| Regression              | Linear   |\n",
        "| Non-negative regression | ReLU     |\n",
        "\n",
        "#### Hidden layers\n",
        "- ReLU is by far the most common choice.\n",
        "  - Faster: more efficient.\n",
        "  - Flat only on negative values: gradient descent works faster.\n",
        "- In the early history, sigmoid was used instead. It is hardly ever used anymore.\n",
        "\n",
        "#### Why activation functions?\n",
        "- Without activation functions, a neural network cannot compute any more complex features than a linear function.\n",
        "  - A linear function of a linear function is itself a linear function.\n",
        "  - The neural network would be equivalent to a linear regression model."
      ],
      "metadata": {
        "id": "2dCMD9ulnWhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient descent\n",
        "Gradient descent is a way to minimize an objective function $J ∈ θ$ parameterized by a model's parameters  $θ ∈ \\mathbb{R}^d$ by updating the parameters in the opposite direction of the graient of the objective function w.r.t. to the parameters.\n",
        "\n",
        "The learning rate $\\eta$ determines the size of the steps we take to reach a (local) minimum. In other words, we follow the direction of the slope of the surface created by the objective function downhill until we reach a valley.\n",
        "\n",
        "### Gradient descent variants\n",
        "There are three variantes of gradient descent, which differ in **how much data** we use to compute the gradient of the objective function. Depending on the amount of data, we make a trade-off between the *accuracy* of the parameter update and the *time* it takes to perform an update.\n",
        "\n",
        "#### Batch Gradient Descent\n",
        "Vanilla gradient descent (batch gradient descent) computes the gradient of the cost function with reference to the parameters $\\theta$ for the entire training set:\n",
        "\n",
        "$$ \\theta = \\theta - \\triangledown_{\\theta}J(θ)$$\n",
        "\n",
        "The gradients for the whole dataset must be calculated to perfom just one update, which can be very slow and intractable for large datasets that don't fit in memory. Moreover, it doesn't allow us to update the model online (with new examples on-the-fly).\n",
        "\n",
        "#### Stochastic Gradient Descent (SGD)\n",
        "Performs a parameter update for each training example $x^{(i)}$ and label $y^{(i)}$:\n",
        "\n",
        "$$\\theta = \\theta -\\eta \\cdot \\triangledown_{\\theta}J(\\theta ; x^{(i)};y^{(i)}) $$\n",
        "\n",
        "Batch gradient descent performs redundant computation for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD performs one update at a time. It performs frequent udpates with a high variance that cause the objective function to fluctuate heavily.\n",
        "\n",
        "![](https://ruder.io/content/images/2016/09/sgd_fluctuation.png)\n",
        "\n",
        "#### Mini-batch gradient descent\n"
      ],
      "metadata": {
        "id": "hHqvONYanj3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax Regression\n",
        "- A generalization of the logistic regression algorithm.\n",
        "- Used to carry out multiclass classification problems.\n",
        "\n",
        "What is the chance of $y$ being any of the given classes?\n",
        "\n",
        "$z_j=\\vec{w_j}\\cdot\\vec{x}+b_j,\\quad j=1,\\ldots,N$\n",
        "\n",
        "$a_j=\\frac{e^{z}_j}{\\sum_{k=1}^{N}{e^{z}_{k}}}=P(y=j|\\vec{x})$\n",
        "\n",
        "### Loss function: Sparse Categorical Crossentropy\n",
        "- **Categorical**: $y$ is classified into categories\n",
        "- **Sparse**: $y$ can only take on one of these categories"
      ],
      "metadata": {
        "id": "XAqDCr3y7lGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization algorithm: Adam\n",
        "- **Ada**ptive **M**oment Estimation\n",
        "- Automatically increase or decrease the learning rate $\\alpha$ depending on how gradient descent is proceeding to get to the minimum faster\n",
        "- Uses a different learning rate for every parameter of the model\n",
        "\n",
        "Intuition\n",
        "- If $w_j$ keeps moving in the same direction, increase $\\alpha_j$.\n",
        "- If $w_j$ keeps oscillating, reduce $\\alpha_j$."
      ],
      "metadata": {
        "id": "GA5mN8obcuXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-label Classification\n",
        "There are several different labales associated with a single input.\n",
        "\n",
        "\n",
        "- Approach 1: treat situation as separate machine learning problems\n",
        "- Approach 2: train one NN with all outputs, use sigmoid activation function for each node in output layer"
      ],
      "metadata": {
        "id": "vcsxjCnLaxh9"
      }
    }
  ]
}